{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Machine Learning with Python_**\n",
    "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed. There are many ML techniques, being the major:\n",
    "* Regression/estimation: predict continuous values\n",
    "* Classification: predicting the item class/category of a case\n",
    "* Clustering: finding the structure of data; summarization\n",
    "* Associations: associating frequent co-occurring items/events\n",
    "* Anomaly detection: discovering abnormal and uniusal cases\n",
    "* Sequence mining: predicting next events\n",
    "* Dimension redunction: reducing the size of data (PCA)\n",
    "* Recommendation systems: recommending items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Supervised vs Unsupervised_**\n",
    "## Supervised Learning\n",
    "Are models that are teached with data (in a dataset), and with that knowledge it can predict unkown or future instances. \n",
    "\n",
    "There are two types of supervised learning techniques: classificationi is the process of predicting discrete class labelss or categories, regression is the process of predicting a continuous values. \n",
    "\n",
    "## Unsupervised learning \n",
    "The model works on its own to dicover information that may not be visible to the human eye. The unsupervised algorithm trains on the dataset and draws conclusions on *unlabeled data*. The unsupervised learning techniques are: dimension reduction (reduce redundant features to make the classification easier), density estimation (find some structuring on the data), market basket analysis (buying some products increases the probability of buying another) and clustering (grouping of data points or objects that are somehow similar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Regression_**\n",
    "Is used to predict continuous values (depending variable, continuous) using other variables (independent variables, categorical or continuous).  There are many algorithms of regression:\n",
    "![linear regression algorithm types](img/img1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "***\n",
    "### Simple Linear Regression\n",
    "Linear regressi√≥n is the approximation of a linear model used to describe the relationship between two or more variables, a dependent variale and an independent variable. The key in the linear regression is that the dependent variable must be continuous. The simple linear regression follows this formulae:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1x$$ \n",
    "\n",
    "where $y$ is the response variable and $x$ the predictor. The objetive of this algorithm is to find the best fit line for the data (determined by the $\\beta_i$). When comparing the observed data with the predicted data there will be an error or residual error (the distance between the prediction line and the points observed). The mean squared errors shows the overall fitness of the model:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted values and $y_i$ are the observed values. The linear regression model yields the line that minimizes the MSE through the $\\beta_i$. \n",
    "\n",
    "This results in a fast and highly interpretable algorithm.\n",
    "\n",
    "\n",
    "\n",
    "### Multiple Linear Regression\n",
    "In the case where there are used multiple independent variables to predict the de dependent variable (which is a continuous variable), it is called multiple linear regression. This algorithm is useful when we want to understand the independent variables effectiveness on prediction. It also can be used for predicting impacts of changes (estimate the effect on the dependent variable given a change in the dependent variables).\n",
    "\n",
    "Its form is:\n",
    "$$\\hat{y} = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_jx_j$$ \n",
    "\n",
    "where $x_i$, where $i=1,2,...,j$, are the predictors used as independent variables.\n",
    "\n",
    "The main idea is to find the best fit hiperplane for the data observed, minimizing the error of the prediction, using Ordinary Least Squares (OLS, using linear algebra operations, but this technique is not efficient: if there are more than 10k rows, don't use it) or using an optimization algorithm (like gradient descent for very large dataset). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Regression\n",
    "***\n",
    "The relations between variables does not always follow a line, there are a richer set of functions that may fit better this kind of non-linear relations. \n",
    "\n",
    "One set of this kind of functions are the polynomial regression, that follow this function:\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x^2+ \\beta_2 x^3+\\beta_{n-1} x^n$$\n",
    "\n",
    "This polynomial regression functions as a speciall case of linear regression ($x_2=x^2, x_3=x^3,...$). Thus, the objective is also minimize the errors, using OLS. So this are not non-linear regression.\n",
    "\n",
    "A non-linear regression is used to model non-linear relationships between dependent variable and a ser of independent variables. $\\hat{y}$ must be a non-linear function of the paramentes $\\beta_i$, not necessarily the features $x$. Examples could be:\n",
    "$$\\hat{y} = log(\\beta_0 + \\beta_1 x^2+ \\beta_2 x^3+\\beta_{n-1} x^n)$$\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1^2 x$$\n",
    "$$\\hat{y} = \\frac{\\beta_0}{1+\\beta_1^{x-\\beta_2}}$$\n",
    "\n",
    "Here, the use of OLS is not possible, and finding the parameters is more difficult. To determine wheather there is a non-linear relation we can instect it visually, if there is a correlation <0.7 it is posible that there is a non-linear relation. \n",
    "\n",
    "If there is a non-linear regression in the data, you should use: \n",
    "* Polynomial regression\n",
    "* Non-linear regression model\n",
    "* Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Apporaches\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test on the Same Dataset\n",
    "When estimating a model we want to use the most accurate model to predict an unknown case. In this approach we use the whole dataset to train our model, and then select a subset the independent vatiables to estimate the dependent variable. We compare the actual values with the predicted values. There are many metrics to estimate accuracy:\n",
    "$$Error = \\frac{1}{n}\\sum_{i = 1}^{n} |y_i-\\hat{y}_i|$$\n",
    "\n",
    "This evaluation approach will most likely yield a high training accuracy but a low out-of-sample accuracy. You must consider that a very high training accuracy could result of over-fitting, damaging the ability of the algorithm to predict out of sample observations (non-generalized model). It is importantn to implorove out of sample accuracy, depending on the goal of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split evaluation approach\n",
    "In this approach we divide the dataset into a train set and a test set. The model is build in the training set, and is evaluated in the test set (predicted values vs actual values). This will generate a more accurate evaluation on out-of-sample accuracy, because the testing set is not used to train the model. This approach has problems due to its dependency on the train and test sets, so we can solve this using K-fold cross-validation.\n",
    "\n",
    "#### K-fold Cross-validation\n",
    "Whe divide the dataset in k parts (or folds), and use K-1 parts to train the model and the other to test it. The algorithm of K-fold cross-validtion stores the parameters from each iteration (changing the fold used as test) and averages this to give a result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics in Regression Models\n",
    "***\n",
    "There are various evaluation metrics:\n",
    "1. Meand absolute error (MAE): is just the average error\n",
    "$$MAE = \\frac{1}{n}\\sum_{i = 1}^{n} |y_i-\\hat{y}_i|$$\n",
    "\n",
    "2. Mean squared errors (MSE): the focus is geared more towards large errors\n",
    "$$MSE = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "3. Root mean squared errors (RMSE): is more interpretable in the same units as the $y$ variable\n",
    "\n",
    "$$MSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n} (y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "4. Relative Absolute Error (residual sum of square):\n",
    "$$ RAE = \\frac{\\sum_{i = 1}^{n}|y_i-\\hat{y}_i|}{\\sum_{i = 1}^{n}|y_i-\\overline{y}|}$$\n",
    "\n",
    "5. Relative squared error: used  by the data science community as it is used for calculating the $R^2$:\n",
    "$$ RSE = \\frac{\\sum_{i = 1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i = 1}^{n}(y_i-\\overline{y})^2}$$\n",
    "\n",
    "$$R^2 = 1-RSE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _**Classification**_\n",
    "Classification is a supervised learning apporach, which can be thought of as a means of categorizing or classifying some unknown items into a discrete set of classes. The target attribute is a categorical variable with discrete values. Classification determines the class label for an unlabeled test case. These are the classification algorithms in ML:\n",
    "\n",
    "![classification algos](img/img2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours (KNN)\n",
    "***\n",
    "As from a set of variables for various observation we can classify a new observation by the closest neighbours according to the set of independent variables (e.g. the 1st NN is the nearest observation point that is already classified as from the independent varibles, and which determines in this case the classfication of the new observation). \n",
    "\n",
    "So, the KNN algorithm uses the K nearest labeled points to classify unlabeled points (according to the preditor set of variables). There are various ways to calculate the distance betweent points (e.g. Euclidian distance). The algorithm follows these steps:\n",
    "1. Pick a value for K\n",
    "2. Calculate the distance of unknown case from all cases\n",
    "3. Select the K-observations in the training data that are \"nearest\" to the unknown data point\n",
    "4. Predict the response of the unknown data poin using the most popular respone value from the KNN\n",
    "\n",
    "To calculate the similarity between two data points we could use: \n",
    "$$Dis(x_1,x_2) = \\sqrt{\\sum_{i=0}^{n}(x_{1i}-x_{2i})^2}$$\n",
    "\n",
    "A lower value of K could produce an overfit model (which is very sensible to noise), making a unfit model for prediction. In the other side of the spectrum, a very big K yields an over generalized model. One solution is reserving a part of the data for accuracy calculation, then starting with K=1 run the model changing K, and select the K with the highest accuracy.\n",
    "\n",
    "The KNN algorithm can also be used to predict a continuous varible, where the K-nearest neightboors mean serves as the predicted value of the objective variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics in Classification\n",
    "We divide the dataset into training set and test set, calculate the model and compare the predicted labels with the actual labels in the test set. There are many evaluation metrics but the core are:\n",
    "\n",
    "1. Jaccard index (0, poor model; 1 perfect model): $$J(y,\\hat{y})= \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|} = \\frac{|y \\cap \\hat{y}|}{|y|+|\\hat{y}|-|y \\cap \\hat{y}|}$$\n",
    "\n",
    "![conj](img/img3.JPG)\n",
    "\n",
    "2. F1-score of the confusion matrix. The higher the F1-score the higher the accuracy (0,1):\n",
    "* Precision = $\\frac{TP}{TP+FP}$\n",
    "* Recall = $\\frac{TP}{TP+FN}$\n",
    "$$ F1-score = 2 \\frac{precision*recall}{precision+recall}$$\n",
    "\n",
    "![confmatrix](img/img4.JPG)\n",
    "\n",
    "3. Log loss: performance of a classifier where the predicted output is a probability value between 0 and 1. Here the classifier with the lower log loss is more accurate: \n",
    "$$ Log loss = \\frac{1}{n}\\sum_{i=1}^{n}(y_i*log(\\hat{y}_i)+(1-y_i)*log(1-\\hat{y}_i))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "***\n",
    "Decision trees are built by splitting the training set into distinct node, where one node contrains all of or most of one category of the data. Each internal node corresponds to a test and each branch corresponds to a result of the test, and each leaf node assigns a classification. The process of building a decision tree is :\n",
    "1. Choose an attribute from your dataset\n",
    "2. Calculate the significance of attribute in splitting of data\n",
    "3. Split the data based on the value of the best attribute\n",
    "4. Repeat all the cycle for each of the attributes\n",
    "\n",
    "Decision trees are built using recursive partitioning to classify the data. The algorithm chooses the most predictive feature to split the data on. What is important in making a decision tree is to determine which attribute is the best or more predictive to split data based on the feature. Thus, more predictiveness means less imputity (i.e. more frequence of the same class) and lower entropy (more information from which make the decision). A node in the tree is considered pure if in 100% of the cases the nodes fall into a specific category of the target field. This method uses recursive partitioning to split the training records into segments by minimizing the impurity at each step (impurity of nodes is calculated by entropy of data in the node). The entropy is calculated with the distribution of the classes: the less uniform the distribution the less the entropy:\n",
    "$$Entropy = -p(A)*log(p(A))-p(B)*log(p(B))$$ \n",
    "\n",
    "So, the algorithm selects each predictive variable as the attributo from which split the branches and selects the one which yields the less entropy (presents the higher information gain, which increases the level of certainty after splitting). The information gain can be calculated as: \n",
    "$$ IG = Entropy_{before}-Entropy_{after}$$\n",
    "![dt](img/img5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "***\n",
    "Ligistic regression is a classification algorithm for categorical variables based on the values of the input fields. It is analogous to linear regression, but it tries to predict a categorical variable instead. The preditors should be continuous, if categorical, they should be coded as dummies. \n",
    "\n",
    "The cases where you should use logistic regression are:\n",
    "* If your target field is binary: 0/1, YES/NO, True/False\n",
    "* If you need probabilistic results\n",
    "* When you need a linear decision boundary \n",
    "\n",
    "![graph](img/img6.JPG)\n",
    "\n",
    "* If you need to understand the impact of a feature\n",
    "\n",
    "The form of the model is:\n",
    "$$ \\hat{y} = P(y=1|X) $$ \n",
    "where $X$ is the set of predictors. \n",
    "\n",
    "The use of linear regression for predicting categorical values results improper due to that model's limitations (the fit of the hiper plane of the linear regression produces  a wide range of values, so is needed a threshold to classify, which results in a loss of information as all values above that threshold are taken as the same). \n",
    "\n",
    "To solve this, the logistic regression uses the sigmoid function (aka logistic function), which only yields values between 0 and 1, and functions like:\n",
    "\n",
    "$$\\sigma(\\beta^TX)=\\frac{1}{1+e^{-\\beta^TX}}= P(y=1|X)$$\n",
    "\n",
    "![sigmoid](img/img7.JPG)\n",
    "\n",
    "The steps followed by the algorith are:\n",
    "1. Initialize de vetor *$\\beta$* with random values.\n",
    "2. Calculate $\\hat{y}=\\sigma(\\beta^TX)$ for an observation\n",
    "3. Compare the predicted value with the actual output and record the error\n",
    "4. Calculate the error for all customers. With this input, calculate the cost function\n",
    "5. Change the *$\\beta$* to reduce the cost\n",
    "6. Iterate until you have minimum cost (one way is through gradient descent, and you can stop training by calculating the accuracy of your model and stop or when it is satisfactory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General cost function\n",
    "We start from:\n",
    "$$ Cost(\\hat{y},y)=\\frac{1}{2} (\\sigma(\\beta^TX) - y)^2$$ \n",
    "And for all the observations (MSE):\n",
    "$$J(\\beta)=\\frac{1}{m}\\sum_{i=1}^{m}cost(\\hat{y},y)$$\n",
    "\n",
    "This function has a complex behavior, making it difficult to find a global minimum and thus the best parameters (because is difficult to calculate the derivative of the function). So, we have to approach this in a different way: we want a functio for which if the desired outcome is want (the observed value of the predicted varible is 1) and the predicted value is 1, the cost funcion must be almost 0, and grow if the predicted value nears 0. Thus we use the $-log(x)$ function:\n",
    "\n",
    "$Cost(\\hat{y},y)= -log(\\hat{y})$ if $y=1$\n",
    "\n",
    "$Cost(\\hat{y},y)= -log(1-\\hat{y})$ if $y=0$\n",
    "\n",
    "Resulting in:\n",
    "$$J(\\beta)= -\\frac{1}{m}\\sum_{i=1}^{m}y_i*log(\\hat{y}_i)+(1-y_i)*log(1-\\hat{y}_i)$$\n",
    "\n",
    "We minimize the cost function using gradient descent (technique to use the derivative of a cost function to change the parameter values, in order to minimize the cost):\n",
    "\n",
    "![i](img/img8.JPG) \n",
    "![i](img/img9.JPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suport Vector Machines\n",
    "***\n",
    "SVM is a supervised algorithm that classifies cases by finding a separator. It:\n",
    "1. Maps data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable.\n",
    "2. Find a separator: the data should be transformed in such a way that a separator could be drawn as a hyperplane. The SVM algorithm outputs an optimal hyperplane that categorized new examplees:\n",
    "\n",
    "![k](img/img11.JPG)\n",
    "\n",
    "In this case we are transfering a one-dimensional non-linearly separable data into a two-dimensional space, where the hyperplane is a line dividing the plane into two parts where each class lays on either side. Mapping data into a higher-dimensional space is called kernelling. The mathematical function used for the trnasformation is known as the kernel function, and can be of different types, such as linear, olynomual, Radial Basis Function (RBF) and sigmoid. Each has its own characteristics, pros and cons and its equation. We choose different kernell functions and compare the results.\n",
    "\n",
    "One reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two classes. Thus, the goal is to choose a hyperplane with as big a margin as possible. Examples closest to the hyperplane are support vectors, so we try to find the hyperplane in such a way that it has the maximum distance to support vectors\n",
    "\n",
    "![k](img/img12.JPG)\n",
    "\n",
    "This optimization problem can be solved using gradient descent. The output of the algorithm are the values $W$ and $b$. The you can calculate wheather a point is above or below the hyperplane. \n",
    "\n",
    "The two main advantages are:\n",
    "1. Accurate in high-dimensional spaces\n",
    "2. Memory efficient: use subset of training points on the decision function called support vectors\n",
    "\n",
    "The disadvantages are:\n",
    "1. The algorithm is prone for overfitting if the number of features is much greater than the number of samples.\n",
    "2. It don't provide probability estimates, which are desriable in most classification problems.\n",
    "3. They are not very efficient computationally if your dataset is very big, such when you have more than 1000 rows.\n",
    "\n",
    "It is useful for image analysis tasks (image classification and hand written digit recognition), text mining tasks (e.g. detecting spam, text category assignment and sentiment analysis), gene expression data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _**Clustering**_\n",
    "Clustering is classifing datapoints with similar characteristics in the cluster, and dissimilar to data points in other clusters. Clustering is useful for:\n",
    "* Exploratory data analysis \n",
    "* Summary generation\n",
    "* Outlier detection\n",
    "* Finding duplicates \n",
    "* Pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorighms\n",
    "***\n",
    "* Partitioned-based clustering: relatively efficient (e.g. k-Means, k-Median, Fuzzy c-Means) for medium and large size databases\n",
    "* Hierarchical clustering: produces trees of clusters (e.g. agglomerative and diviseve), are very intuitive and generally used for small datasets\n",
    "* Density-based clustering: produces arbitrary shaped clusters, good for dealing with spatial clusters or when there is noise in your data set (e.g. DB scan algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "***\n",
    "K-means divides the data into k non-overlapping subsets (clusters) without anyy cluster-internal structure. Examples within a cluster are very similar and examples across the different clusters are vry different. Thus, the intra-cluster distances are minimized and the inter-cluster distances are maximized.\n",
    "\n",
    "The distance can be calculed using a specific type of Mankowski distance:\n",
    "$$Dis(x_1,x_2)= \\sqrt{\\sum_{i=0}^{n}(x_{1i}-x_{2i})^2}$$\n",
    "\n",
    "We have to normalize the data to get a precise dissimilarity measure. There are many different dissimilarity measures, which must be used depending on the data you're using. It follows:\n",
    "\n",
    "1. The algorithm starts with k centrois located randomly in the features space\n",
    "2. Distance calculation: we assign each data point to each centroid (forming the distance matrix)\n",
    "3. Assign each point to the closest centroid. The idea it to minimize this distance between data points and centroids (min the sum of the squared differences): \n",
    "$$ SSE = \\sum_{i=1}^{n}(x_i-C_j)^2$$\n",
    "\n",
    "4. Compute the new centroids for each cluster: we update the centroids to e the mean for data points in its cluster\n",
    "5. Iterate steps 2-4 until the centroids no longer move (until the algorithm converges)\n",
    "\n",
    "Because it is an euristic algorithm, there is no guarantee that it will converge to the global optimum (i.e. it could converge to local optimums). Thus is necessary to run the process multiple times, with different starting points.\n",
    "\n",
    "### Error calculation\n",
    "Because this is an unsupervised learning algorithm, an external approach (compare the culsters with the ground thruth) may not be available. So we can calculate the average distance between datapoints within a cluster as a metric of error.\n",
    "\n",
    "### Choosing k\n",
    "The correct coice fo k is often ambiguous because it's very dependent on the shape and scale of the distribution of points in a dataset. One technique commonly used is to run the clistering across the different values of k and looking at a metric of acuracy for clustering (e.g. mean distance of data points to cluter centroid). The problem is that with increasing K this distance will be reduced, so we need to find the elbow point (where the rate of decrease sharply shifts):\n",
    "\n",
    "![elbow](img/img13.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "***\n",
    "Hierarchical clustering algorithms build a hierarchy of clusters where each node is a cluster consists of the clusters of its daughter nodes. Strategies for hierarchical clustering generally fall into two types: divisive (top down, you start with all observations in a large cluster and breack it down to into smaller pieces) and agglomerative (bottom up, where each observation starts in its own cluster and pairs of clusters are merged together as they move up the hierarchy):\n",
    "\n",
    "![img14](img/img14.jpg)\n",
    "\n",
    "The agglomerative algorithm follows:\n",
    "1. Create $n$ clusters, one for each data point\n",
    "2. Compute the proximity matrix \n",
    "3. Repeat: \n",
    "  1. Merge the two closest clusters\n",
    "  2. Update the proximity matrix\n",
    "4. Stop until only a single cluster remains with the results stored in a dendrogram\n",
    "\n",
    "We can calculate the distance with the euclidean distance:\n",
    "\n",
    "$$Dis(x_1,x_2)= \\sqrt{\\sum_{i=0}^{n}(x_{1i}-x_{2i})^2}$$\n",
    "\n",
    "To calculate the distance between clusters there are different criteria:\n",
    "+ Single-Linkage Clustering: minimum distance between clusters\n",
    "+ Complete-Linkage Clustering: maximum distance between clusters\n",
    "+ Average Linkage Clustering: average distance between clusters\n",
    "+ Centroid Linkage Clustering: distance between cluster centroids\n",
    "\n",
    "The selection completely depends on the data type, dumensionality of data and most importantly, the domain knowledge of the data set. Different approaches to defining the distance between clusters distinguish the different algorithms.\n",
    "\n",
    "![img15](img/img15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density-Based Spacial Clusterin of Applications with Noise)\n",
    "It is a density-based clustering algorithm which is appropiate to use when examining spatial data. A difference with k-means is that density-based clustering locates regions of high density, and separates outliers, while k-means assigns all points to a cluster even if they do not belong in any. Density in this context is the number of points in a specified radius. Thus, it can find arbitrarily shaped clusters\n",
    "\n",
    "It works based on two parameters: radius and minimum points. R determines a specified radius that if it includes enough points within it, we call it a dense area. M determines the minimum number of data points we want in a neighborhoodto define a cluster.\n",
    "\n",
    "A data point is a core point if within our neighborhood of the point there are at least M points. A data point is a border point if its neighbourhood contains less than M data points or if it is reachable from some core point. Reachability means it is within our distance from a core point. An outlier is a point that is not a core point and also is not close enough to be reachable from a core point. \n",
    "\n",
    "The next step is to connect core points that are neighbors and put them in the same cluster. Thus, a cluster is formed as at least one core point plus all reachable core points plus all their borders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommendation Engines\n",
    "Recommender systems capture the pattern of peoples' behavior and use it to predict what else they might like or want. Recommender engines are used in social networks, streaming services, food menus, web personalized expirience. This results in a broader exposure to more products, possibility of continual usage or purchase of products, and provides a better experience. \n",
    "\n",
    "THere are two main types of recommending systems, which are differentiated by the type of statement that a consumer might make: \n",
    "+ Content-based: show me more of the same of what I've liked before (similar items)\n",
    "+ Collaborative filtering: tell me what's popular among my neighbors, I also maght like it (similar groups of users and provide recommendations based on similar tastes within that group). The challenges of collaborative filtering is data sparsity (isers in general rate only a limited number of items), cold start (difficulty in recommentdation to new users or new items), scalability (increase in number of users or items). There are two appoaches to this:\n",
    "  + User-based collaboratie filtering: based on users' neighborhood (searches for similar users to predict the preferences of the active user)\n",
    "  + Ite-based collaborative filtering: based in items' similarity\n",
    "\n",
    "There are also hybrid recommender systems. There are two apporaches:\n",
    "1. Memory-based: uses the entire user-item dataset to generate a recommendation and uses statistical techniques to approximate users or items (e.g. pearson correlation, euclidian distance, ect)\n",
    "2. Model-based: develops a model of users in an attempt to learn their preferences. Models can be created using ML techniques like regression, clustering, classification, etc. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
